{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-04-14T12:44:52.849367Z","iopub.status.busy":"2023-04-14T12:44:52.848761Z","iopub.status.idle":"2023-04-14T12:44:52.860666Z","shell.execute_reply":"2023-04-14T12:44:52.859649Z","shell.execute_reply.started":"2023-04-14T12:44:52.849320Z"},"trusted":true},"outputs":[],"source":["# PATH = '/kaggle/input/utilities-cifar10'\n","\n","# import sys\n","\n","# sys.path.insert(1, PATH)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:44:52.870290Z","iopub.status.busy":"2023-04-14T12:44:52.869906Z","iopub.status.idle":"2023-04-14T12:44:53.990514Z","shell.execute_reply":"2023-04-14T12:44:53.989388Z","shell.execute_reply.started":"2023-04-14T12:44:52.870251Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\malay\\miniconda3\\envs\\cs776\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd \n","import random \n","import torch\n","import torchvision\n","from torchvision import transforms, datasets\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader,random_split\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import time\n","\n","from ae_cifar10 import AutoEncoder\n","from utils import fgsm_attack, pgd_linf, to_numpy_array_cifar10, EarlyStopping\n","from vgg import VGG"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:52:12.989312Z","iopub.status.busy":"2023-04-14T12:52:12.988796Z","iopub.status.idle":"2023-04-14T12:52:12.994796Z","shell.execute_reply":"2023-04-14T12:52:12.993509Z","shell.execute_reply.started":"2023-04-14T12:52:12.989265Z"},"trusted":true},"outputs":[],"source":["attack_type = 'fgsm'\n","activation_type = 'gelu'\n","lr = 0.001\n","batch_size = 16\n","num_epochs = 10\n","dataset = 'cifar10'\n","# z_dim = 10\n","include_noise = True\n","\n","# FGSM parameters\n","eps_fgsm = 5e-3\n","#PGD parameters\n","eps_pgd, alpha, num_iter = 5e-3, 1e-2, 5"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:52:14.675939Z","iopub.status.busy":"2023-04-14T12:52:14.675067Z","iopub.status.idle":"2023-04-14T12:52:15.593450Z","shell.execute_reply":"2023-04-14T12:52:15.592341Z","shell.execute_reply.started":"2023-04-14T12:52:14.675899Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n"]}],"source":["# CIFAR10 Dataset\n","transform = transforms.Compose(\n","    [transforms.Resize((224, 224)),\n","     transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","transform_ae = transforms.Compose(\n","    [transforms.Resize((32, 32))])\n","\n","transform_classifier = transforms.Compose(\n","    [transforms.Resize((224, 224))])\n","\n","dataset = datasets.CIFAR10(root= './data', train = True, download =True, transform = transform)\n","# len(dataset)\n","train_dataset, val_dataset = torch.utils.data.random_split(dataset, [40000, 10000])"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:52:16.083962Z","iopub.status.busy":"2023-04-14T12:52:16.083585Z","iopub.status.idle":"2023-04-14T12:52:16.090032Z","shell.execute_reply":"2023-04-14T12:52:16.088880Z","shell.execute_reply.started":"2023-04-14T12:52:16.083925Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Selected device: cuda\n"]}],"source":["device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(f'Selected device: {device}')"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:52:16.836032Z","iopub.status.busy":"2023-04-14T12:52:16.835283Z","iopub.status.idle":"2023-04-14T12:52:19.091894Z","shell.execute_reply":"2023-04-14T12:52:19.090637Z","shell.execute_reply.started":"2023-04-14T12:52:16.835980Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["classifier = VGG()\n","classifier.to(device)\n","classifier.load_state_dict(torch.load('./models/vgg16.pth.tar')['vgg16_state_dict'])"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:52:19.094658Z","iopub.status.busy":"2023-04-14T12:52:19.094249Z","iopub.status.idle":"2023-04-14T12:52:19.106651Z","shell.execute_reply":"2023-04-14T12:52:19.105394Z","shell.execute_reply.started":"2023-04-14T12:52:19.094617Z"},"trusted":true},"outputs":[{"data":{"text/plain":["AutoEncoder(\n","  (encoder): Sequential(\n","    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): GELU(approximate='none')\n","    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): GELU(approximate='none')\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (6): GELU(approximate='none')\n","    (7): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (8): GELU(approximate='none')\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (decoder): Sequential(\n","    (0): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n","    (1): GELU(approximate='none')\n","    (2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (3): GELU(approximate='none')\n","    (4): ConvTranspose2d(32, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (5): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (6): GELU(approximate='none')\n","    (7): ConvTranspose2d(16, 16, kernel_size=(2, 2), stride=(2, 2))\n","    (8): GELU(approximate='none')\n","    (9): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (10): GELU(approximate='none')\n","    (11): ConvTranspose2d(16, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (12): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): GELU(approximate='none')\n","  )\n",")"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# if activation_type == 'gelu':\n","#     convAE = Autoencoder_GELU(device, include_noise)\n","# else:\n","#     convAE = Autoencoder_RELU(device, include_noise)\n","\n","convAE = AutoEncoder(device, include_noise)\n","convAE.to(device)\n","# Loss function\n","criterion = nn.MSELoss()\n","# Optimizer\n","optimizer = optim.Adam(convAE.parameters(), lr=lr)\n","# Early stopping criteria\n","early_stopping = EarlyStopping()\n","# Check the model\n","convAE"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 16, 32, 32]             448\n","              GELU-2           [-1, 16, 32, 32]               0\n","            Conv2d-3           [-1, 32, 32, 32]           4,640\n","              GELU-4           [-1, 32, 32, 32]               0\n","         MaxPool2d-5           [-1, 32, 16, 16]               0\n","            Conv2d-6           [-1, 32, 16, 16]          25,632\n","              GELU-7           [-1, 32, 16, 16]               0\n","            Conv2d-8           [-1, 64, 16, 16]          51,264\n","              GELU-9           [-1, 64, 16, 16]               0\n","        MaxPool2d-10             [-1, 64, 8, 8]               0\n","  ConvTranspose2d-11           [-1, 32, 16, 16]           8,224\n","             GELU-12           [-1, 32, 16, 16]               0\n","           Conv2d-13           [-1, 32, 16, 16]          25,632\n","             GELU-14           [-1, 32, 16, 16]               0\n","  ConvTranspose2d-15           [-1, 16, 16, 16]          12,816\n","           Conv2d-16           [-1, 16, 16, 16]           6,416\n","             GELU-17           [-1, 16, 16, 16]               0\n","  ConvTranspose2d-18           [-1, 16, 32, 32]           1,040\n","             GELU-19           [-1, 16, 32, 32]               0\n","           Conv2d-20           [-1, 16, 32, 32]           2,320\n","             GELU-21           [-1, 16, 32, 32]               0\n","  ConvTranspose2d-22            [-1, 3, 32, 32]           1,203\n","           Conv2d-23            [-1, 3, 32, 32]              84\n","             GELU-24            [-1, 3, 32, 32]               0\n","================================================================\n","Total params: 139,719\n","Trainable params: 139,719\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 2.13\n","Params size (MB): 0.53\n","Estimated Total Size (MB): 2.68\n","----------------------------------------------------------------\n"]}],"source":["from torchsummary import summary\n","\n","summary(convAE, (3, 32, 32))"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:52:20.509842Z","iopub.status.busy":"2023-04-14T12:52:20.509108Z","iopub.status.idle":"2023-04-14T12:52:20.517136Z","shell.execute_reply":"2023-04-14T12:52:20.515858Z","shell.execute_reply.started":"2023-04-14T12:52:20.509801Z"},"trusted":true},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:52:21.493718Z","iopub.status.busy":"2023-04-14T12:52:21.493333Z","iopub.status.idle":"2023-04-14T12:52:21.499753Z","shell.execute_reply":"2023-04-14T12:52:21.498589Z","shell.execute_reply.started":"2023-04-14T12:52:21.493684Z"},"trusted":true},"outputs":[],"source":["def load_checkpoint(checkpoint):\n","    convAE.load_state_dict(checkpoint['convAE_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    return checkpoint['train_losses'], checkpoint['val_losses']\n","\n","def save_checkpoint(checkpoint, model_path):\n","    torch.save(checkpoint, model_path)"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:52:23.257272Z","iopub.status.busy":"2023-04-14T12:52:23.256854Z","iopub.status.idle":"2023-04-14T12:52:23.274639Z","shell.execute_reply":"2023-04-14T12:52:23.273459Z","shell.execute_reply.started":"2023-04-14T12:52:23.257234Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(11, 11)"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["load_model = True\n","if include_noise:\n","    file_path = f'./models/ae_cifar10_{attack_type}_{activation_type}_proposed.pth.tar'\n","else:\n","    file_path = f'./models/ae_cifar10_{attack_type}_{activation_type}_vanilla.pth.tar'\n","\n","train_losses, val_losses = [], []\n","\n","if load_model:\n","    train_losses, val_losses = load_checkpoint(torch.load(file_path))\n","\n","len(train_losses), len(val_losses)"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1/10, train_loss: 0.0821, val_loss: 0.0810,           time_taken: 35.72 mins\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2920\\507592377.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mrec_imgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvAE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madv_imgs_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrec_imgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\malay\\OneDrive\\Desktop\\CSE\\Semester 2\\CS776A\\dlcv_project\\ae_cifar10.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minclude_noise\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m             \u001b[0mencoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoded\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m         \u001b[0mdecoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["start_time = time.time()\n","\n","for epoch in range(num_epochs):\n","    train_loss, val_loss = 0.0, 0.0\n","\n","    for _, (imgs, labels) in enumerate(train_loader):\n","        # print(imgs.shape)\n","        batch_size = imgs.shape[0]\n","        imgs, labels = Variable(imgs.to(device), requires_grad=True), Variable(labels.to(device))\n","        \n","        if attack_type == 'fgsm':\n","            adv_imgs, _ = fgsm_attack(classifier, imgs, labels, eps_fgsm, dataset)\n","        else:\n","            adv_imgs, _ = pgd_linf(classifier, imgs, labels, eps_pgd, alpha, num_iter)\n","            \n","        adv_imgs = adv_imgs.to(device)\n","        imgs, adv_imgs_ = transform_ae(imgs), transform_ae(adv_imgs).to(device)\n","\n","        optimizer.zero_grad()\n","        rec_imgs = convAE.forward(adv_imgs_)\n","        loss = criterion(imgs, rec_imgs)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()*batch_size\n","\n","    for _, (imgs, labels) in enumerate(val_loader):\n","        # print(imgs.shape)\n","        batch_size = imgs.shape[0]\n","        imgs, labels = Variable(imgs.to(device), requires_grad=True), Variable(labels.to(device))\n","\n","        if attack_type == 'fgsm':\n","            adv_imgs, _ = fgsm_attack(classifier, imgs, labels, eps_fgsm, dataset)\n","        else:\n","            adv_imgs, _ = pgd_linf(classifier, imgs, labels, eps_pgd, alpha, num_iter)\n","\n","        adv_imgs = adv_imgs.to(device)\n","        imgs, adv_imgs_ = transform_ae(imgs), transform_ae(adv_imgs).to(device)\n","        # adv_imgs = adv_imgs.to(device)\n","\n","        optimizer.zero_grad()\n","        rec_imgs = convAE.forward(adv_imgs_)\n","        loss = criterion(imgs, rec_imgs)\n","        loss.backward()\n","        optimizer.step()\n","        val_loss += loss.item()*batch_size\n","    \n","    train_loss, val_loss = train_loss/len(train_dataset), val_loss/len(val_dataset)\n","\n","    train_losses.append(train_loss)\n","    val_losses.append(val_loss)\n","\n","    print(f'Epoch: {epoch+1}/{num_epochs}, train_loss: {train_loss:.4f}, val_loss: {val_loss:.4f}, \\\n","          time_taken: {(time.time()-start_time)/60:.2f} mins')\n","    \n","    checkpoint = {\n","        'convAE_state_dict': convAE.state_dict(), 'optimizer_state_dict': optimizer.state_dict(),\n","        'train_losses': train_losses, 'val_losses': val_losses\n","    }\n","\n","    save_checkpoint(checkpoint, file_path)\n","\n","    early_stopping(train_loss, val_loss)\n","    if early_stopping.early_stop:\n","        print(\"Early Stopping critieria satisfied\")\n","        break"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.16"}},"nbformat":4,"nbformat_minor":4}
