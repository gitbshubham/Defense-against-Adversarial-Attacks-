{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\malay\\miniconda3\\envs\\cs776\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random \n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "from ae_cifar10 import AutoEncoder\n",
    "from utils import fgsm_attack, pgd_linf, to_numpy_array_cifar10, EarlyStopping\n",
    "from vgg import VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_type = 'fgsm'\n",
    "activation_type = 'gelu'\n",
    "lr = 0.001\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "dataset = 'cifar10'\n",
    "# z_dim = 10\n",
    "include_noise = True\n",
    "\n",
    "# FGSM parameters\n",
    "eps_fgsm = 5e-3\n",
    "#PGD parameters\n",
    "eps_pgd, alpha, num_iter = 5e-3, 1e-2, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# CIFAR10 Dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((224, 224)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "transform_ae = transforms.Compose(\n",
    "    [transforms.Resize((32, 32))])\n",
    "\n",
    "transform_classifier = transforms.Compose(\n",
    "    [transforms.Resize((224, 224))])\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root= './data', train = False, download =True, transform = transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Selected device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = VGG()\n",
    "classifier.to(device)\n",
    "classifier.load_state_dict(torch.load('./models/vgg16.pth.tar')['vgg16_state_dict'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Evaluation on our proposed architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include_noise = True\n",
    "\n",
    "if include_noise:\n",
    "    file_path = f'./models/ae_cifar10_{attack_type}_{activation_type}_proposed.pth.tar'\n",
    "else:\n",
    "    file_path = f'./models/ae_cifar10_{attack_type}_{activation_type}_vanilla.pth.tar'\n",
    "\n",
    "convAE = AutoEncoder(device, include_noise)\n",
    "convAE.to(device)\n",
    "convAE.load_state_dict(torch.load(file_path)['convAE_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, total = 0, 0\n",
    "\n",
    "for _, (imgs, labels) in enumerate(test_loader):\n",
    "    batch_size = imgs.shape[0]\n",
    "    imgs, labels = Variable(imgs.to(device), requires_grad=True), Variable(labels.to(device))\n",
    "    \n",
    "    if attack_type == 'fgsm':\n",
    "        adv_imgs, _ = fgsm_attack(classifier, imgs, labels, eps_fgsm, dataset)\n",
    "    else:\n",
    "        adv_imgs, _ = pgd_linf(classifier, imgs, labels, eps_pgd, alpha, num_iter)\n",
    "\n",
    "    adv_imgs = adv_imgs.to(device)\n",
    "    adv_imgs_ = transform_ae(adv_imgs).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rec_imgs = convAE(adv_imgs_)\n",
    "        rec_imgs = transform_classifier(rec_imgs)\n",
    "        \n",
    "    y_preds = classifier(rec_imgs).argmax(dim=1)\n",
    "    correct += (y_preds==labels).sum().item()\n",
    "    total += labels.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.005, Test Accuracy: 0.5716\n",
      "5716 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Epsilon: {}, Test Accuracy: {}\".format(eps_fgsm, correct / total))\n",
    "print(f'{correct} {total}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Evaluation on Vanilla Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include_noise = False\n",
    "\n",
    "if include_noise:\n",
    "    file_path = f'./models/ae_cifar10_{attack_type}_{activation_type}_proposed.pth.tar'\n",
    "else:\n",
    "    file_path = f'./models/ae_cifar10_{attack_type}_{activation_type}_vanilla.pth.tar'\n",
    "\n",
    "convAE = AutoEncoder(device, include_noise)\n",
    "convAE.to(device)\n",
    "convAE.load_state_dict(torch.load(file_path)['convAE_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, total = 0, 0\n",
    "\n",
    "for _, (imgs, labels) in enumerate(test_loader):\n",
    "    batch_size = imgs.shape[0]\n",
    "    imgs, labels = Variable(imgs.to(device), requires_grad=True), Variable(labels.to(device))\n",
    "    \n",
    "    if attack_type == 'fgsm':\n",
    "        adv_imgs, _ = fgsm_attack(classifier, imgs, labels, eps_fgsm, dataset)\n",
    "    else:\n",
    "        adv_imgs, _ = pgd_linf(classifier, imgs, labels, eps_pgd, alpha, num_iter)\n",
    "\n",
    "    adv_imgs = adv_imgs.to(device)\n",
    "    adv_imgs_ = transform_ae(adv_imgs).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rec_imgs = convAE(adv_imgs_)\n",
    "        rec_imgs = transform_classifier(rec_imgs)\n",
    "        \n",
    "    y_preds = classifier(rec_imgs).argmax(dim=1)\n",
    "    correct += (y_preds==labels).sum().item()\n",
    "    total += labels.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.005, Test Accuracy: 0.5654\n"
     ]
    }
   ],
   "source": [
    "print(\"Epsilon: {}, Test Accuracy: {}\".format(eps_fgsm, correct / total))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_time = 0\n",
    "\n",
    "for _, (imgs, labels) in enumerate(test_loader):\n",
    "    batch_size = imgs.shape[0]\n",
    "    imgs, labels = Variable(imgs.to(device), requires_grad=True), Variable(labels.to(device))\n",
    "\n",
    "    if attack_type == 'fgsm':\n",
    "        adv_imgs, _ = fgsm_attack(classifier, imgs, labels, eps_fgsm, dataset)\n",
    "    else:\n",
    "        adv_imgs, _ = pgd_linf(classifier, imgs, labels, eps_pgd, alpha, num_iter)\n",
    "\n",
    "    attack_comp = time.time()\n",
    "\n",
    "    adv_imgs = adv_imgs.to(device)\n",
    "    adv_imgs_ = transform_ae(adv_imgs).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rec_imgs = convAE(adv_imgs_)\n",
    "        rec_imgs = transform_classifier(rec_imgs)\n",
    "        \n",
    "    y_preds = classifier(rec_imgs).argmax(dim=1)\n",
    "    tot_time += time.time() - attack_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for defense against a single instance of attack: 0.0004 sec\n",
      "Total time taken: 0 min 4.38 sec\n"
     ]
    }
   ],
   "source": [
    "print(f'Time taken for defense against a single instance of attack: {(tot_time)/len(test_dataset):.4f} sec')\n",
    "print(f'Total time taken: {tot_time//60:.0f} min {tot_time%60:.2f} sec')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs776",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
